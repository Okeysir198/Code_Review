{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d7f251a",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5f820a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0940713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# print(os.environ.get('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fffbc8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['evaluation',\n",
       " 'testcases',\n",
       " 'langgraph.json',\n",
       " 'notebooks',\n",
       " '.DS_Store',\n",
       " '00_simple_fastrtc.py',\n",
       " '.env',\n",
       " '.langgraph_api',\n",
       " 'logs',\n",
       " 'src',\n",
       " 'llm_app.py',\n",
       " '.gitignore',\n",
       " 'test_graph',\n",
       " '=12.3',\n",
       " 'audio_samples',\n",
       " 'asset',\n",
       " 'app_config.py',\n",
       " '.git',\n",
       " '__pycache__',\n",
       " 'docs',\n",
       " 'langgraph copy.json']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"../../../HF_models\")\n",
    "os.listdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa63a43",
   "metadata": {},
   "source": [
    "### Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e34c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForSpeechSeq2Seq, AutoProcessor\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "# Load model components separately\n",
    "torch_dtype = torch.float16\n",
    "# MODEL_NAME = \"../../../HF_models/whisper-large-v3-turbo\"\n",
    "MODEL_NAME =\"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16\n",
    "            )\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch_dtype, \n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=quantization_config\n",
    "    )\n",
    "\n",
    "# Create pipeline\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor\n",
    ")\n",
    "\n",
    "# Use the proper parameter structure\n",
    "audio_path = \"../audio_samples/20250210-181617_8783_27728204887-all.mp3\"\n",
    "result = pipe(\n",
    "    audio_path,\n",
    "    decoder_kwargs={\"language\": \"en\"},\n",
    "    generate_kwargs={\"task\": \"transcribe\"},\n",
    "    return_timestamps=True\n",
    "    \n",
    ")\n",
    "\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814b12d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"../../../HF_models/whisper-large-v3-turbo\"\n",
    "# # model.save_pretrained(MODEL_NAME)\n",
    "# processor.save_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f9ec1f",
   "metadata": {},
   "source": [
    "### Test STT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75632375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20250218-172808_8562_27727903218-all.mp3',\n",
       " '20250218-172457_8788_27832667859-all.mp3',\n",
       " '20250219-093730_8688_27786753396-all.mp3',\n",
       " '20250210-181617_8783_27728204887-all.mp3',\n",
       " '20250220-083952_8689_27648301967-all.mp3']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"../audio_samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61a5819d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 16:15:27,325 - WARNING - No optimal config for openai/whisper-large-v3-turbo, using defaults\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-06-13 16:15:33 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-06-13 16:15:33 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    use_lhotse: true\n",
      "    skip_missing_manifest_entries: true\n",
      "    input_cfg: null\n",
      "    tarred_audio_filepaths: null\n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    shuffle: true\n",
      "    num_workers: 2\n",
      "    pin_memory: true\n",
      "    max_duration: 40.0\n",
      "    min_duration: 0.1\n",
      "    text_field: answer\n",
      "    batch_duration: null\n",
      "    use_bucketing: true\n",
      "    bucket_duration_bins: null\n",
      "    bucket_batch_size: null\n",
      "    num_buckets: 30\n",
      "    bucket_buffer_size: 20000\n",
      "    shuffle_buffer_size: 10000\n",
      "    \n",
      "[NeMo W 2025-06-13 16:15:33 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    use_lhotse: true\n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    max_duration: 40.0\n",
      "    min_duration: 0.1\n",
      "    num_workers: 2\n",
      "    pin_memory: true\n",
      "    text_field: answer\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-06-13 16:15:33 nemo_logging:393] PADDING: 0\n",
      "[NeMo I 2025-06-13 16:15:35 nemo_logging:393] Using RNNT Loss : tdt\n",
      "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n",
      "[NeMo I 2025-06-13 16:15:35 nemo_logging:393] Using RNNT Loss : tdt\n",
      "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n",
      "[NeMo I 2025-06-13 16:15:35 nemo_logging:393] Using RNNT Loss : tdt\n",
      "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n",
      "[NeMo I 2025-06-13 16:15:36 nemo_logging:393] Model EncDecRNNTBPEModel was successfully restored from /home/ct-admin/.cache/huggingface/hub/models--nvidia--parakeet-tdt-0.6b-v2/snapshots/c4b828d094af2c7238dfe03b58e0c56bc69ea57a/parakeet-tdt-0.6b-v2.nemo.\n"
     ]
    }
   ],
   "source": [
    "from src.STT import create_stt_model\n",
    "from app_config import CONFIG\n",
    "# print(CONFIG['stt'])\n",
    "\n",
    "# Configure the model\n",
    "config1 = CONFIG['stt'].copy()\n",
    "config1[\"openai/whisper-large-v3-turbo\"]['beam_size']=3\n",
    "config1[\"model_name\"] = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "config2 = CONFIG['stt'].copy()\n",
    "config2[\"model_name\"] = \"nvidia/parakeet-tdt-0.6b-v2\"\n",
    "\n",
    "# Create the model\n",
    "model1 = create_stt_model(config1)\n",
    "model2 = create_stt_model(config2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8924fc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription1:  Good day, how are you? Good day. Good, my name is Priti, calling from Cartrack Accounts Department. Can I please speak to Phineas Mthongo? Yes, you must again. I can note that a call should be recorded for quality and security purposes. Can you please confirm your ID number, sir? Thank you very much. So there is an alcohol justice because of the outstanding amount of 288,000 and 30 cents. And we need an immediate payment to be done today. Do you maybe have enough funds for us to do the deduction? What about that? Sorry? What are you saying? Yes. Yes. We need an immediate payment to be done today. Okay. Yes. Do you have enough funds so that you can do the deduction? No funds. Sorry? No money. When can you be able to make the payment? Next month. Next month on which day? On the 4th. On the? 4th. On the 4th, so a double deduction. Yeah, on the 4th. No problem. I'll make a debit order then for the 4th and then you're going to get an SMS from the bank asking you to authorize the mandate. Listen so that you authorize it and also there's going to be a send run for submission fee okay yes and then your salary taxes on which day sir on networking okay and then is there any information that you like to update on the account no all right and then what is your email address, Mr. Mthongo? Mahalamsongo.com. Mahalamsongo.com. Small letters, right? Yes. Oh, okay. No problem then, sir. So do you have an alternative number? No. All right. So please ensure that you do make the payment to avoid contract suspending the services you're getting, including having access to the app and positioning of your VHS through control room. Okay. All right. Do you have any questions? No. No problem. So we'll do the deduction then of the amount of $288,000 on the 40th of next month, and your monthly subscription is going to be deducted on the same day also. Okay. No problem then sir. Thank you very much and for your time. No. Alright. Okay. Thank you. Alright. Bye. Thank you.\n",
      "[NeMo I 2025-06-13 16:15:48 nemo_logging:393] Timestamps requested, setting decoding timestamps to True. Capture them in Hypothesis object,                         with output[0][idx].timestep['word'/'segment'/'char']\n",
      "[NeMo I 2025-06-13 16:15:48 nemo_logging:393] Using RNNT Loss : tdt\n",
      "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-06-13 16:15:48 nemo_logging:405] `include_duration` is not implemented for CUDA graphs\n",
      "Transcribing: 100%|██████████| 1/1 [00:00<00:00,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Transcription2: Good day, how are you? Good day. I'm good. My name is Prithi Colling from Catrak Accounts Department. Can I please speak to Vineyard Sumtongo? Can not that call to be recorded for quality and security purposes? Can you please confirm your IT number, sir? 742263843. Thank you very much. So there is an accordion because of the outstanding amount of 288 run in Delicent and 288 run yes and this is and we need an immediate payment to be done today. Do you maybe have enough funds so that you can do the deduction? Come on. Sorry? What are you saying? The account is behind with an amount of two hundred and eighty eight run in this end sir. Yes. Yes, we need an immediate payment to be done today. Okay. Yes. Do you have enough funds so that you can do the deduction? No found. Sorry? No. No money. When can you be able to make the payment? Next month. Next month on which day? On the fourth. On the fourth. On the fourth, so a double deduction. Yeah, on the fourth. No problem. I'll make it a bit older than for the fourth and then you're going to get an SMS from the bank asking you to authorize the mandate. Please ensure that you authorize it and also there's going to be a 10-run for submission fee. Okay. Yes, and then your salary data on which day, sir? On networking. Okay, and then is there any information that you like to update on the account? No. Alright, and then what is your email address, Mr. Mtongo? Mahalam Songho Agin.com. Mahalam Song Agent Method Com Small Letters, right? Yes. Oh, okay. No problem then, sir. So do you have an alternative number? No. Alright, so please ensure then that you do make the payments to avoid cut track suspending the services you're getting, including having access to the app and positioning of your VICL through control room. Okay. Alright, do you have any question? No. No problem, sir. So we'll do the deduction then of the amount of 288 running 37 on the fourth of next month and your monthly subscription is going to be deducted on the same day also. Okay. No problem then sir. Thank you very much then for your time. No. Alright. Thank you. Alright, bye. Thank you.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Transcribe an audio file\n",
    "audio_path = \"../audio_samples/20250210-181617_8783_27728204887-all.mp3\"\n",
    "\n",
    "result1 = model1.transcribe(audio_path)\n",
    "print(f\"Transcription1: {result1['text']}\")\n",
    "\n",
    "result2 = model2.transcribe(audio_path)\n",
    "print(\"=\"*50)\n",
    "print(f\"Transcription2: {result2['text']}\")\n",
    "\n",
    "# result2['timestamps']\n",
    "# if 'timestamps' in result2:\n",
    "#     for stamp in result2['timestamps']['segment']:\n",
    "#          print(f\"{stamp['start']:.02f}s - {stamp['end']:.02f}s : {stamp['segment']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729fbae5",
   "metadata": {},
   "source": [
    "### NVIDIA Parakeet-TDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74c94c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "asr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=\"nvidia/parakeet-tdt-0.6b-v2\")\n",
    "\n",
    "audio_path = \"../audio_samples/20250210-181617_8783_27728204887-all.mp3\"\n",
    "\n",
    "output = asr_model.transcribe(audio_path)\n",
    "print(output[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7343ee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"../audio_samples/20250210-181617_8783_27728204887-all.mp3\"\n",
    "\n",
    "output = asr_model.transcribe([audio_path], timestamps=True)\n",
    "# by default, timestamps are enabled for char, word and segment level\n",
    "word_timestamps = output[0].timestamp['word'] # word level timestamps for first sample\n",
    "segment_timestamps = output[0].timestamp['segment'] # segment level timestamps\n",
    "char_timestamps = output[0].timestamp['char'] # char level timestamps\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(output[0].text)\n",
    "print(\"=\"*100)\n",
    "\n",
    "\n",
    "for stamp in segment_timestamps:\n",
    "    print(f\"{stamp['start']:.02f}s - {stamp['end']:.02f}s : {stamp['segment']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ed054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.STT import create_stt_model\n",
    "\n",
    "config = {\n",
    "    \"model_name\": \"nvidia/parakeet-tdt-0.6b-v2\",\n",
    "    \"show_logs\": False,\n",
    "}\n",
    "\n",
    "# Create the model\n",
    "model = create_stt_model(config)\n",
    "\n",
    "# Transcribe an audio file\n",
    "audio_path = \"../audio_samples/20250210-181617_8783_27728204887-all.mp3\"\n",
    "\n",
    "\n",
    "result = model.transcribe(audio_path)\n",
    "\n",
    "\n",
    "# Print the result\n",
    "print(f\"Transcription: {result['text']}\")\n",
    "\n",
    "# # # Print timestamps if available\n",
    "# # if \"timestamps\" in result:\n",
    "# #     for word, start, end in result[\"timestamps\"]:\n",
    "# #         print(f\"'{word}': {start:.2f}s - {end:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a19c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transcribe(audio_path)\n",
    "# Print the result\n",
    "print(f\"Transcription: {result['text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_cv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
